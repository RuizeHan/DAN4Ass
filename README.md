# DAN4Ass
Multi-view Multi-Human Association with Deep Assignment Network, in IEEE TIP 2022.

## New
The datasets used in this work are released to the public.

Synthetic MvMHA Image Dataset (MvMHA-S):
Link: https://pan.baidu.com/s/1GQ7Zy1d2lkYhV0hlT0GUfQ 

PW: MMHA


Real-World MvMHA Image Dataset (MvMHA-R):
Link: https://pan.baidu.com/s/1aR0-7gh4BQNDZ3bT-HR-Wg 

PW: MMHA

Code:
Coming soon...(It is expected early than Apr.


## Introduction
Identifying the same persons across different views plays an important role in many vision applications. In this paper, we study this important problem, denoted as Multi-view MultiHuman Association (MvMHA), on multi-view images that are taken by different cameras at the same time. Different from previous works on human association across two views, this paper is focused on more general and challenging scenarios of more than two views, and none of these views are fixed or priorly known.

In this work, we propose a Deep Assignment Network (DAN) to model the constrained multi-view multi-clique assignment problem, which is implemented by two popular backbone networks (RNN and GNN). On both of them, we verify the effectiveness of the proposed unsupervised constraint loss.


![framework](https://github.com/RuizeHan/DAN4Ass/blob/main/framework.jpg)


This work can be cited by:
```
@ARTICLE{han2022mvmha,  
author={Han, Ruize and Wang, Yun and Yan, Haomin and Feng, Wei and Wang, Song},  
journal={IEEE Transactions on Image Processing},   
title={Multi-View Multi-Human Association With Deep Assignment Network},   
year={2022},  
volume={31},  
number={},  
pages={1830-1840},  
doi={10.1109/TIP.2021.3139178}}
```
